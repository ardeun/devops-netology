# Домашнее задание к занятию 17 «Инцидент-менеджмент»

## Задание

Составьте постмортем на основе реального сбоя системы GitHub в 2018 году.

Информацию о сбое можно изучить по ссылкам ниже:

* [краткое описание на русском языке](https://habr.com/ru/post/427301/);
* [развёрнутое описание на английском языке](https://github.blog/2018-10-30-oct21-post-incident-analysis/).


## Ответ

### Краткое описание инцидента

21 октября 2018 года, в 22:52 несколько сервисов GitHub пострадали от нарушения связности сети и последующих проблем в работе БД. В результате множество пользователей наблюдали непоследовательную информацию на сайте 

### Предшествующие события

21 октября в 22:52 проводились регламентные работы по замене вышедшего из строя оптического оборудования 100G

### Причина инцидента

потеря связности между ядром сети и основным датацентром на восточном побережье США  в результате этого развалились кластеры БД

### Воздействие

пользователи испытывали проблемы при работе с сайтом: не работали Issues, Webhooks, невозможно было создать и разместить странички на GitHub Pages

### Обнаружение

21 октября 22:54 система мониторинга начала слать предупреждения об ошибках в системе. В 23:02 дежурные инженеры определили, что множество кластеров баз данных находились в непридвиденном состоянии

### Реакция

Дежурные инженеры вручную остановили деплой и перевели систему в статус "желтый"

Был подключен координатор по инцидентам. Он изменил статус сервиса на "красный"

Подключили дополнительных инженеров из команды администрирования БД

Отключили часть сервисов для пользователей, чтобы снизить нагрузку на кластер
### Восстановление

Восстановлены из резервных копий данные MySQ и последующей репликации актуальных данных с кластера на западном побережье, возврат к стабильной топологии обслуживания и возвращение в работу отключенных сервисов

### Таймлайн

* 21 22:52 UTC: аварийное переключение кластеров для направления операций записи в ЦОД на западном побережье США
* 21 22:54 UTC: система мониторинга начала слать оповещения о многочисленных ошибках
* 21 23:02 UTC: дежурные инженеры обнаружили что множество кластеров БД оказались в "неожиданном" состоянии
* 21 23:07 UTC: команда заблокировала инструмент развертывания, для предотвращения дополнительных возможных изменений
* 21 23:13 UTC: подключили дополнительных инженеров из команды обслуживания БД
* 21 23:19 UTC: остановка работы веб-хуков и сборку GitHub Pages
* 22 00:05 UTC: разработка плана по возврату кластера БД к нормальному состоянию
* 22 00:41 UTC: начался процесс восстановления из бекапа кластеров MySQL
* 22 06:51 UTC: несколько кластеров завершили восстановление из бекапов в датацентре на восточном побережье и начали репликацию данных с западного побережье
* 22 07:46 UTC: GutHub опубликовали сообщение в блоге, чтобы донести пользователям больше информации
* 22 11:12 UTC: восстановился основной кластер БД на восточном побережье
* 22 13:15 UTC: зафиксирован пик нагрузки трафика на GitHub.com
* 22 16:24 UTC: завершение репликации, возврат к исходной топологии
* 22 16:45 UTC: началась обработка накопившегося стека событий
* 22 23:03 UTC: работа веб-хуков и GutHub Pages восстановлена, подтверждена целостность и правильная работа всех систем 
### Последующие действия

- Настройка конфигурации Orchestrator чтобы предотвратит передачу основных баз данных между регионами
- Переход на новую систему оповещения о статусе системы, оповещать о статусе отдельных компонентах сервиса.
- улучение отказоустойчивости, чтобы система оставалась доступной даже при отказе одного из датацентров.
- Компания займёт более проактивную позицию в проверке предположений
- Компания будет инвестировать больше в chaos engineering, тестируя различные сценарии отказа


